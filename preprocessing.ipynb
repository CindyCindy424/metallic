{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import *\n",
    "from scipy.ndimage import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.hub\n",
    "import os\n",
    "from torchvision.datasets import *\n",
    "import random\n",
    "import torch.utils.data as Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = r'D:\\onedrive\\eyisheng\\OneDrive\\files\\大三下\\科研\\金属分类\\data'\n",
    "DATA_ROOT_CORROSION = DATA_ROOT + r'\\corrosion'\n",
    "DATA_ROOT_NONCORROSION = DATA_ROOT + r'\\noncorrosion'\n",
    "\n",
    "DATASET_ROOT = './dataset'\n",
    "DATASET_ROOT_TRAIN = os.path.join(DATASET_ROOT,'train')\n",
    "DATASET_ROOT_TEST = os.path.join(DATASET_ROOT,'test')\n",
    "DATASET_TRAIN_POS = os.path.join(DATASET_ROOT_TRAIN,'1')\n",
    "DATASET_TRAIN_NEG = os.path.join(DATASET_ROOT_TRAIN,'0')\n",
    "DATASET_TEST_POS = os.path.join(DATASET_ROOT_TEST,'1')\n",
    "DATASET_TEST_NEG = os.path.join(DATASET_ROOT_TEST,'0')\n",
    "\n",
    "if not os.path.exists(DATASET_ROOT_TRAIN):\n",
    "    os.mkdir(DATASET_ROOT_TRAIN)\n",
    "\n",
    "if not os.path.exists(DATASET_ROOT_TEST):\n",
    "    os.mkdir(DATASET_ROOT_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### get raw jpg files from directories\n",
    "def getFileNames(is_corrosion=False):\n",
    "    path_files = []\n",
    "    walk_root = DATA_ROOT_NONCORROSION if is_corrosion==False else DATA_ROOT_CORROSION\n",
    "    \n",
    "    for root, dirs, files in os.walk(walk_root):\n",
    "        for f in files:\n",
    "            fpath = os.path.join(root,f)\n",
    "            path_files.append(fpath)\n",
    "            \n",
    "    label = 0 if is_corrosion==False else 1\n",
    "    res = list(map(lambda x:(x,label),path_files))\n",
    "    return res\n",
    "\n",
    "\n",
    "def checkImgShape():\n",
    "    shapes = set()\n",
    "    for line in non_corrosion_files:\n",
    "        jpgfile = Image.open(line[0])\n",
    "        jpgfile = np.array(jpgfile)\n",
    "        shapes.add(jpgfile.shape)\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def resizeImg(src,dsize):\n",
    "    jpgfile = Image.open(src)\n",
    "    jpgfile = np.array(jpgfile)\n",
    "    jpgfile = cv2.resize(jpgfile,dsize)\n",
    "    return jpgfile\n",
    "\n",
    "\n",
    "def preprocessingPipeline(src,target):\n",
    "    img = resizeImg(src,(256,256)) ### image shapes\n",
    "    return [src,target,img]\n",
    "    \n",
    "    \n",
    "def filesToImg(path_files):\n",
    "    train = []\n",
    "    test = []\n",
    "    for line in path_files:\n",
    "        if random.random()<0: ### train test split\n",
    "            test.append( preprocessingPipeline(line[0],line[1]) )\n",
    "        else:\n",
    "            train.append( preprocessingPipeline(line[0],line[1]) )\n",
    "    return {'train':train,'test':test}\n",
    "    \n",
    "    \n",
    "def saveImgToFiles(imgs):\n",
    "    \n",
    "    if not os.path.exists(DATASET_TRAIN_POS):\n",
    "        os.mkdir(DATASET_TRAIN_POS)\n",
    "    if not os.path.exists(DATASET_TRAIN_NEG):\n",
    "        os.mkdir(DATASET_TRAIN_NEG) \n",
    "    if not os.path.exists(DATASET_TEST_POS):\n",
    "        os.mkdir(DATASET_TEST_POS)\n",
    "    if not os.path.exists(DATASET_TEST_NEG):\n",
    "        os.mkdir(DATASET_TEST_NEG)\n",
    "        \n",
    "    for line in imgs['train']:\n",
    "        src = os.path.split(line[0])[-1]\n",
    "        if line[1]==0:\n",
    "            src = os.path.join(DATASET_TRAIN_NEG,src)\n",
    "        elif line[1]==1:\n",
    "            src = os.path.join(DATASET_TRAIN_POS,src)\n",
    "        Image.fromarray(line[2]).save(src)\n",
    "        \n",
    "    for line in imgs['test']:\n",
    "        src = os.path.split(line[0])[-1]\n",
    "        if line[1]==0:\n",
    "            src = os.path.join(DATASET_TEST_NEG,src)\n",
    "        elif line[1]==1:\n",
    "            src = os.path.join(DATASET_TEST_POS,src)\n",
    "        Image.fromarray(line[2]).save(src)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgWriter():\n",
    "    img_data = filesToImg(img_files)\n",
    "\n",
    "    saveImgToFiles(img_data)\n",
    "    return\n",
    "\n",
    "\n",
    "# non_corrosion_files = getFileNames(is_corrosion=False)\n",
    "# corrosion_files = getFileNames(is_corrosion=True)\n",
    "# img_files = corrosion_files\n",
    "# img_files.extend(non_corrosion_files)\n",
    "# del corrosion_files,non_corrosion_files\n",
    "# ImgWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "BATCH_SIZE = 6\n",
    "NUM_CLASS = 2\n",
    "CV = 5\n",
    "\n",
    "P_momentem = 0.8\n",
    "P_lr = 1e-3\n",
    "train_ratio = 0.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "full_dataset = ImageFolder(DATASET_ROOT_TRAIN,transform = data_transform)\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(np.floor( total_size * train_ratio ))\n",
    "test_size = int(total_size - train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\steve/.cache\\torch\\hub\\moskomule_senet.pytorch_master\n"
     ]
    }
   ],
   "source": [
    "hub_model = torch.hub.load(\n",
    "    'moskomule/senet.pytorch',\n",
    "    'se_resnet20',\n",
    "    num_classes=NUM_CLASS,\n",
    "#     pre_trained=True\n",
    ")\n",
    "net = hub_model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=P_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 1.584\n",
      "[1,    10] loss: 2.193\n",
      "[1,    15] loss: 2.707\n",
      "[2,     5] loss: 2.320\n",
      "[2,    10] loss: 2.032\n",
      "[2,    15] loss: 1.876\n",
      "[3,     5] loss: 1.731\n",
      "[3,    10] loss: 1.590\n",
      "[3,    15] loss: 1.507\n",
      "[4,     5] loss: 1.382\n",
      "[4,    10] loss: 1.325\n",
      "[4,    15] loss: 1.351\n",
      "[5,     5] loss: 1.323\n",
      "[5,    10] loss: 1.314\n",
      "[5,    15] loss: 1.327\n",
      "[6,     5] loss: 1.264\n",
      "[6,    10] loss: 1.212\n",
      "[6,    15] loss: 1.180\n",
      "[7,     5] loss: 1.236\n",
      "[7,    10] loss: 1.291\n",
      "[7,    15] loss: 1.358\n",
      "[8,     5] loss: 1.341\n",
      "[8,    10] loss: 1.305\n",
      "[8,    15] loss: 1.281\n",
      "[9,     5] loss: 1.235\n",
      "[9,    10] loss: 1.201\n",
      "[9,    15] loss: 1.176\n",
      "[10,     5] loss: 1.133\n",
      "[10,    10] loss: 1.103\n",
      "[10,    15] loss: 1.078\n",
      "[11,     5] loss: 1.059\n",
      "[11,    10] loss: 1.047\n",
      "[11,    15] loss: 1.038\n",
      "[12,     5] loss: 1.021\n",
      "[12,    10] loss: 1.004\n",
      "[12,    15] loss: 0.995\n",
      "[13,     5] loss: 0.970\n",
      "[13,    10] loss: 0.949\n",
      "[13,    15] loss: 0.932\n",
      "[14,     5] loss: 0.907\n",
      "[14,    10] loss: 0.890\n",
      "[14,    15] loss: 0.878\n",
      "[15,     5] loss: 0.858\n",
      "[15,    10] loss: 0.843\n",
      "[15,    15] loss: 0.833\n",
      "[16,     5] loss: 0.814\n",
      "[16,    10] loss: 0.801\n",
      "[16,    15] loss: 0.788\n",
      "[17,     5] loss: 0.770\n",
      "[17,    10] loss: 0.757\n",
      "[17,    15] loss: 0.747\n",
      "[18,     5] loss: 0.732\n",
      "[18,    10] loss: 0.720\n",
      "[18,    15] loss: 0.710\n",
      "[19,     5] loss: 0.696\n",
      "[19,    10] loss: 0.687\n",
      "[19,    15] loss: 0.681\n",
      "[20,     5] loss: 0.670\n",
      "[20,    10] loss: 0.663\n",
      "[20,    15] loss: 0.658\n",
      "Finished Training\n",
      "Accuracy of  100.000000 \n",
      "Accuracy of  40.000000 \n",
      "[1,     5] loss: 0.108\n",
      "[1,    10] loss: 0.226\n",
      "[1,    15] loss: 0.290\n",
      "[2,     5] loss: 0.411\n",
      "[2,    10] loss: 0.394\n",
      "[2,    15] loss: 0.415\n",
      "[3,     5] loss: 0.432\n",
      "[3,    10] loss: 0.403\n",
      "[3,    15] loss: 0.408\n",
      "[4,     5] loss: 0.387\n",
      "[4,    10] loss: 0.375\n",
      "[4,    15] loss: 0.372\n",
      "[5,     5] loss: 0.378\n",
      "[5,    10] loss: 0.376\n",
      "[5,    15] loss: 0.403\n",
      "[6,     5] loss: 0.434\n",
      "[6,    10] loss: 0.485\n",
      "[6,    15] loss: 0.539\n",
      "[7,     5] loss: 0.540\n",
      "[7,    10] loss: 0.525\n",
      "[7,    15] loss: 0.516\n",
      "[8,     5] loss: 0.517\n",
      "[8,    10] loss: 0.507\n",
      "[8,    15] loss: 0.502\n",
      "[9,     5] loss: 0.485\n",
      "[9,    10] loss: 0.474\n",
      "[9,    15] loss: 0.469\n",
      "[10,     5] loss: 0.460\n",
      "[10,    10] loss: 0.456\n",
      "[10,    15] loss: 0.455\n",
      "[11,     5] loss: 0.443\n",
      "[11,    10] loss: 0.433\n",
      "[11,    15] loss: 0.424\n",
      "[12,     5] loss: 0.424\n",
      "[12,    10] loss: 0.425\n",
      "[12,    15] loss: 0.430\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-20e06891acf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\IDE_Project_Programming\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\IDE_Project_Programming\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_total_loss = []\n",
    "_total_acc = []\n",
    "\n",
    "for cv in range(CV):\n",
    "    \n",
    "    hub_model = torch.hub.load(\n",
    "        'moskomule/senet.pytorch',\n",
    "        'se_resnet20',\n",
    "        num_classes=NUM_CLASS,\n",
    "    #     pre_trained=True\n",
    "    )\n",
    "    net = hub_model\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=P_lr)\n",
    "    \n",
    "    dataset_train, dataset_test = torch.utils.data.random_split(full_dataset, [train_size,test_size])\n",
    "    trainloader = Data.DataLoader(dataset=dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    testloader = Data.DataLoader(dataset=dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    _loss = []\n",
    "    for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            _loss.append(running_loss)\n",
    "            if i % 5 == 4:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, np.sum(_loss)/len(_loss)))\n",
    "    print('Finished Training')\n",
    "\n",
    "    \n",
    "    class_correct = list(0. for i in range(NUM_CLASS))\n",
    "    class_total = list(0. for i in range(NUM_CLASS))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            if c.ndim==0:\n",
    "                label = labels[0]\n",
    "                class_correct[label] += c.item()\n",
    "                class_total[label] += 1\n",
    "                continue\n",
    "            for i in range(np.min([BATCH_SIZE,len(labels)])):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    _acc = []\n",
    "    for i in range(NUM_CLASS):\n",
    "        print('Accuracy of  %f ' % ( 100 * class_correct[i] / class_total[i] ))\n",
    "        _acc.append(100 * class_correct[i] / class_total[i])\n",
    "    \n",
    "    \n",
    "    _total_loss.extend(_loss)\n",
    "    _total_acc.extend(_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(_total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
