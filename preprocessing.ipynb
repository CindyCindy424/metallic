{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import *\n",
    "from scipy.ndimage import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.hub\n",
    "import os\n",
    "from torchvision.datasets import *\n",
    "import random\n",
    "import torch.utils.data as Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = r'D:\\onedrive\\eyisheng\\OneDrive\\files\\大三下\\科研\\金属分类\\data'\n",
    "DATA_ROOT_CORROSION = DATA_ROOT + r'\\corrosion'\n",
    "DATA_ROOT_NONCORROSION = DATA_ROOT + r'\\noncorrosion'\n",
    "\n",
    "DATASET_ROOT = './dataset'\n",
    "DATASET_ROOT_TRAIN = os.path.join(DATASET_ROOT,'train')\n",
    "DATASET_ROOT_TEST = os.path.join(DATASET_ROOT,'test')\n",
    "DATASET_TRAIN_POS = os.path.join(DATASET_ROOT_TRAIN,'1')\n",
    "DATASET_TRAIN_NEG = os.path.join(DATASET_ROOT_TRAIN,'0')\n",
    "DATASET_TEST_POS = os.path.join(DATASET_ROOT_TEST,'1')\n",
    "DATASET_TEST_NEG = os.path.join(DATASET_ROOT_TEST,'0')\n",
    "\n",
    "if not os.path.exists(DATASET_ROOT_TRAIN):\n",
    "    os.mkdir(DATASET_ROOT_TRAIN)\n",
    "\n",
    "if not os.path.exists(DATASET_ROOT_TEST):\n",
    "    os.mkdir(DATASET_ROOT_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### get raw jpg files from directories\n",
    "def getFileNames(is_corrosion=False):\n",
    "    path_files = []\n",
    "    walk_root = DATA_ROOT_NONCORROSION if is_corrosion==False else DATA_ROOT_CORROSION\n",
    "    \n",
    "    for root, dirs, files in os.walk(walk_root):\n",
    "        for f in files:\n",
    "            fpath = os.path.join(root,f)\n",
    "            path_files.append(fpath)\n",
    "            \n",
    "    label = 0 if is_corrosion==False else 1\n",
    "    res = list(map(lambda x:(x,label),path_files))\n",
    "    return res\n",
    "\n",
    "\n",
    "def checkImgShape():\n",
    "    shapes = set()\n",
    "    for line in non_corrosion_files:\n",
    "        jpgfile = Image.open(line[0])\n",
    "        jpgfile = np.array(jpgfile)\n",
    "        shapes.add(jpgfile.shape)\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def resizeImg(src,dsize):\n",
    "    jpgfile = Image.open(src)\n",
    "    jpgfile = np.array(jpgfile)\n",
    "    jpgfile = cv2.resize(jpgfile,dsize)\n",
    "    return jpgfile\n",
    "\n",
    "\n",
    "def preprocessingPipeline(src,target):\n",
    "    img = resizeImg(src,(256,256)) ### image shapes\n",
    "    return [src,target,img]\n",
    "    \n",
    "    \n",
    "def filesToImg(path_files):\n",
    "    train = []\n",
    "    test = []\n",
    "    for line in path_files:\n",
    "        if random.random()<0: ### train test split\n",
    "            test.append( preprocessingPipeline(line[0],line[1]) )\n",
    "        else:\n",
    "            train.append( preprocessingPipeline(line[0],line[1]) )\n",
    "    return {'train':train,'test':test}\n",
    "    \n",
    "    \n",
    "def saveImgToFiles(imgs):\n",
    "    \n",
    "    if not os.path.exists(DATASET_TRAIN_POS):\n",
    "        os.mkdir(DATASET_TRAIN_POS)\n",
    "    if not os.path.exists(DATASET_TRAIN_NEG):\n",
    "        os.mkdir(DATASET_TRAIN_NEG) \n",
    "    if not os.path.exists(DATASET_TEST_POS):\n",
    "        os.mkdir(DATASET_TEST_POS)\n",
    "    if not os.path.exists(DATASET_TEST_NEG):\n",
    "        os.mkdir(DATASET_TEST_NEG)\n",
    "        \n",
    "    for line in imgs['train']:\n",
    "        src = os.path.split(line[0])[-1]\n",
    "        if line[1]==0:\n",
    "            src = os.path.join(DATASET_TRAIN_NEG,src)\n",
    "        elif line[1]==1:\n",
    "            src = os.path.join(DATASET_TRAIN_POS,src)\n",
    "        Image.fromarray(line[2]).save(src)\n",
    "        \n",
    "    for line in imgs['test']:\n",
    "        src = os.path.split(line[0])[-1]\n",
    "        if line[1]==0:\n",
    "            src = os.path.join(DATASET_TEST_NEG,src)\n",
    "        elif line[1]==1:\n",
    "            src = os.path.join(DATASET_TEST_POS,src)\n",
    "        Image.fromarray(line[2]).save(src)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgWriter():\n",
    "    img_data = filesToImg(img_files)\n",
    "\n",
    "    saveImgToFiles(img_data)\n",
    "    return\n",
    "\n",
    "\n",
    "# non_corrosion_files = getFileNames(is_corrosion=False)\n",
    "# corrosion_files = getFileNames(is_corrosion=True)\n",
    "# img_files = corrosion_files\n",
    "# img_files.extend(non_corrosion_files)\n",
    "# del corrosion_files,non_corrosion_files\n",
    "# ImgWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 6\n",
    "NUM_CLASS = 2\n",
    "CV = 5\n",
    "\n",
    "P_momentem = 0.8\n",
    "P_lr = 1e-3\n",
    "train_ratio = 0.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "full_dataset = ImageFolder(DATASET_ROOT_TRAIN,transform = data_transform)\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(np.floor( total_size * train_ratio ))\n",
    "test_size = int(total_size - train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_model = torch.hub.load(\n",
    "    'moskomule/senet.pytorch',\n",
    "    'se_resnet20',\n",
    "    num_classes=NUM_CLASS,\n",
    "#     pre_trained=True\n",
    ")\n",
    "net = hub_model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=P_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\steve/.cache\\torch\\hub\\moskomule_senet.pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 1.577\n",
      "[1,    10] loss: 2.273\n",
      "[1,    15] loss: 2.757\n",
      "[2,     5] loss: 2.421\n",
      "[2,    10] loss: 2.281\n",
      "[2,    15] loss: 2.290\n",
      "[3,     5] loss: 2.112\n",
      "[3,    10] loss: 2.031\n",
      "[3,    15] loss: 2.027\n",
      "[4,     5] loss: 1.875\n",
      "[4,    10] loss: 1.766\n",
      "[4,    15] loss: 1.699\n",
      "[5,     5] loss: 1.616\n",
      "[5,    10] loss: 1.541\n",
      "[5,    15] loss: 1.507\n",
      "[6,     5] loss: 1.422\n",
      "[6,    10] loss: 1.363\n",
      "[6,    15] loss: 1.320\n",
      "[7,     5] loss: 1.249\n",
      "[7,    10] loss: 1.203\n",
      "[7,    15] loss: 1.169\n",
      "[8,     5] loss: 1.117\n",
      "[8,    10] loss: 1.092\n",
      "[8,    15] loss: 1.083\n",
      "[9,     5] loss: 1.050\n",
      "[9,    10] loss: 1.027\n",
      "[9,    15] loss: 1.012\n",
      "[10,     5] loss: 0.977\n",
      "[10,    10] loss: 0.967\n",
      "[10,    15] loss: 0.966\n",
      "[11,     5] loss: 0.942\n",
      "[11,    10] loss: 0.921\n",
      "[11,    15] loss: 0.903\n",
      "[12,     5] loss: 0.875\n",
      "[12,    10] loss: 0.857\n",
      "[12,    15] loss: 0.846\n",
      "[13,     5] loss: 0.824\n",
      "[13,    10] loss: 0.808\n",
      "[13,    15] loss: 0.797\n",
      "[14,     5] loss: 0.794\n",
      "[14,    10] loss: 0.782\n",
      "[14,    15] loss: 0.774\n",
      "[15,     5] loss: 0.765\n",
      "[15,    10] loss: 0.753\n",
      "[15,    15] loss: 0.746\n",
      "[16,     5] loss: 0.733\n",
      "[16,    10] loss: 0.725\n",
      "[16,    15] loss: 0.723\n",
      "[17,     5] loss: 0.709\n",
      "[17,    10] loss: 0.699\n",
      "[17,    15] loss: 0.689\n",
      "[18,     5] loss: 0.675\n",
      "[18,    10] loss: 0.665\n",
      "[18,    15] loss: 0.656\n",
      "[19,     5] loss: 0.643\n",
      "[19,    10] loss: 0.634\n",
      "[19,    15] loss: 0.627\n",
      "[20,     5] loss: 0.615\n",
      "[20,    10] loss: 0.606\n",
      "[20,    15] loss: 0.598\n",
      "Finished Training\n",
      "Accuracy of  100.000000 \n",
      "Accuracy of  100.000000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\steve/.cache\\torch\\hub\\moskomule_senet.pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 1.763\n",
      "[1,    10] loss: 2.498\n",
      "[1,    15] loss: 2.961\n",
      "[2,     5] loss: 2.431\n",
      "[2,    10] loss: 2.035\n",
      "[2,    15] loss: 1.832\n",
      "[3,     5] loss: 1.656\n",
      "[3,    10] loss: 1.594\n",
      "[3,    15] loss: 1.624\n",
      "[4,     5] loss: 1.529\n",
      "[4,    10] loss: 1.451\n",
      "[4,    15] loss: 1.436\n",
      "[5,     5] loss: 1.359\n",
      "[5,    10] loss: 1.301\n",
      "[5,    15] loss: 1.282\n",
      "[6,     5] loss: 1.217\n",
      "[6,    10] loss: 1.174\n",
      "[6,    15] loss: 1.150\n",
      "[7,     5] loss: 1.099\n",
      "[7,    10] loss: 1.070\n",
      "[7,    15] loss: 1.048\n",
      "[8,     5] loss: 1.036\n",
      "[8,    10] loss: 1.013\n",
      "[8,    15] loss: 1.007\n",
      "[9,     5] loss: 0.997\n",
      "[9,    10] loss: 0.996\n",
      "[9,    15] loss: 1.015\n",
      "[10,     5] loss: 0.997\n",
      "[10,    10] loss: 0.980\n"
     ]
    }
   ],
   "source": [
    "_total_loss = []\n",
    "_total_acc = []\n",
    "\n",
    "for cv in range(CV):\n",
    "    \n",
    "    hub_model = torch.hub.load(\n",
    "        'moskomule/senet.pytorch',\n",
    "        'se_resnet20',\n",
    "        num_classes=NUM_CLASS,\n",
    "    #     pre_trained=True\n",
    "    )\n",
    "    net = hub_model\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=P_lr)\n",
    "    \n",
    "    dataset_train, dataset_test = torch.utils.data.random_split(full_dataset, [train_size,test_size])\n",
    "    trainloader = Data.DataLoader(dataset=dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    testloader = Data.DataLoader(dataset=dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    _loss = []\n",
    "    for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            _loss.append(running_loss)\n",
    "            if i % 5 == 4:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, np.pow(np.sum(np.pow(np.array(_loss),2)),0.5)/len(_loss)))\n",
    "    print('Finished Training')\n",
    "\n",
    "    \n",
    "    class_correct = list(0. for i in range(NUM_CLASS))\n",
    "    class_total = list(0. for i in range(NUM_CLASS))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            if c.ndim==0:\n",
    "                label = labels[0]\n",
    "                class_correct[label] += c.item()\n",
    "                class_total[label] += 1\n",
    "                continue\n",
    "            for i in range(np.min([BATCH_SIZE,len(labels)])):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    _acc = []\n",
    "    for i in range(NUM_CLASS):\n",
    "        print('Accuracy of  %f ' % ( 100 * class_correct[i] / class_total[i] ))\n",
    "        _acc.append(100 * class_correct[i] / class_total[i])\n",
    "    \n",
    "    \n",
    "    _total_loss.append(_loss)\n",
    "    _total_acc.append(_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(_total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
